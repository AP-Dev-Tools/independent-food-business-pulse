name: Weekly FHRS Data Update

permissions:
  contents: write

on:
  workflow_dispatch: {}
  repository_dispatch:
    types: [run-fhrs-update]
  schedule:
    - cron: '0 9 * * 1'   # Mondays 09:00 UTC

jobs:
  update-data:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0          # needed for rebase/pull to work reliably

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests

      - name: Download FHRS data
        run: python download_fhrs_data.py

      # ðŸ§· Keep a copy of the previous LA totals before they get overwritten
      - name: Snapshot previous LA totals (if any)
        run: |
          mkdir -p data
          if [ -f data/la_totals_last.json ]; then
            cp data/la_totals_last.json data/__prev_la_totals.json
            echo "Saved previous LA totals to data/__prev_la_totals.json"
          else
            echo "No previous la_totals_last.json found (first run is fine)."
          fi

      - name: Process data and identify new businesses
        run: python process_fhrs_data.py

      # âœ… Build Top-10 LA growth/reductions using the pre-run snapshot as 'previous'
      - name: Build LA deltas (Top-10 growth/reductions)
        run: |
          python - <<'PY'
          import json, os
          from datetime import date

          CUR_PATH  = "data/la_totals_last.json"      # written by your process script
          PREV_PATH = "data/__prev_la_totals.json"    # our pre-run snapshot (if it existed)
          OUT_PATH  = "data/la_deltas_latest.json"

          def load(path):
            if not os.path.exists(path): return None
            with open(path, "r", encoding="utf-8") as f:
              return json.load(f)

          cur  = load(CUR_PATH)
          prev = load(PREV_PATH) or {}

          # If we still don't have current, write an empty scaffold and exit
          if not cur:
            out = {
              "date": str(date.today()),
              "by_sector": {
                "MOBILE":{"growth":[],"reductions":[]},
                "RESTAURANT_CAFE":{"growth":[],"reductions":[]},
                "PUB_BAR":{"growth":[],"reductions":[]},
                "TAKEAWAY":{"growth":[],"reductions":[]},
                "HOTEL":{"growth":[],"reductions":[]},
                "OTHER":{"growth":[],"reductions":[]},
              }
            }
            os.makedirs("data", exist_ok=True)
            with open(OUT_PATH,"w",encoding="utf-8") as f: json.dump(out, f, ensure_ascii=False)
            print("[warn] Missing current LA totals; wrote empty scaffold.")
            raise SystemExit(0)

          # Accept either shape:
          # A) { "LA": {"MOBILE":n, "RESTAURANT_CAFE":m, ...}, ... }
          # B) { "MOBILE":{"LA":n,...}, "RESTAURANT_CAFE":{"LA":m,...}, ... }
          SECTORS = ["MOBILE","RESTAURANT_CAFE","PUB_BAR","TAKEAWAY","HOTEL","OTHER"]

          def normalize(d):
            if not d: return {}
            # Detect shape
            if set(d.keys()) & set(SECTORS):
              # shape B -> A
              out = {}
              for s in SECTORS:
                for la, v in (d.get(s) or {}).items():
                  out.setdefault(la, {})[s] = int(v or 0)
              return out
            # already shape A (per-LA at top)
            return d

          curN  = normalize(cur)
          prevN = normalize(prev)

          by_sector = {}
          for s in SECTORS:
            growth, reductions = [], []
            all_las = set(curN.keys()) | set(prevN.keys())
            for la in all_las:
              cv = int((curN.get(la) or {}).get(s, 0))
              pv = int((prevN.get(la) or {}).get(s, 0))
              delta = cv - pv
              if delta > 0:
                growth.append({"la": la, "delta": delta, "current": cv})
              elif delta < 0:
                reductions.append({"la": la, "delta": delta, "current": cv})
            growth.sort(key=lambda x: x["delta"], reverse=True)
            reductions.sort(key=lambda x: x["delta"])  # most negative first
            by_sector[s] = {"growth": growth[:10], "reductions": reductions[:10]}

          out = {"date": str(date.today()), "by_sector": by_sector}
          os.makedirs("data", exist_ok=True)
          with open(OUT_PATH, "w", encoding="utf-8") as f:
            json.dump(out, f, ensure_ascii=False)
          print(f"[ok] Wrote {OUT_PATH}")
          PY

      - name: Commit processed data
        run: |
          set -euo pipefail
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config user.name  "github-actions[bot]"
          git config --global --add safe.directory "$GITHUB_WORKSPACE"

          # Stage everything under /data
          git add -A data || true

          if git diff --staged --quiet; then
            echo "No changes to commit."
            exit 0
          fi

          git commit -m "Update dashboard + per-LA deltas"

          # Rebase on latest main, tolerating local changes
          git fetch origin
          git pull --rebase --autostash origin main || true

          git push origin HEAD:main

      - name: Summary
        run: |
          echo "âœ… Update finished."
          echo "Core artifacts:"
          ls -lh data/dashboard_data.json data/latest_snapshot.json data/cumulative_index.json data/seen_ids.txt.gz 2>/dev/null || true
          echo "Per-type CSVs (if any):"
          ls -lh data/cumulative/*/*.csv 2>/dev/null || true
          echo "LA delta artifacts:"
          ls -lh data/la_deltas_* data/la_deltas_latest.json data/la_totals_last.json 2>/dev/null || true
