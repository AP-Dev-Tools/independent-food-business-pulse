name: Weekly FHRS Data Update

permissions:
  contents: write

on:
  workflow_dispatch: {}
  repository_dispatch:
    types: [run-fhrs-update]
  schedule:
    - cron: '0 9 * * 1'  # Mondays 09:00 UTC

jobs:
  update-data:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests

      # 1) Try your existing downloader first
      - name: Download FHRS data (custom script)
        run: |
          set -e
          python download_fhrs_data.py || true

      # 2) Fallback: fetch per-LA XMLs if none were produced
      - name: Fallback fetch of FHRS XML (per LA)
        run: |
          set -e
          mkdir -p data/raw
          # EDIT this list to the LAs you track
          declare -a LA_IDS=(223 1 226)  # Bromsgrove, Birmingham, Worcester
          # Only fetch if nothing exists yet
          if [ -z "$(ls -1 data/raw/*.xml 2>/dev/null)" ]; then
            echo "No XMLs found from custom script. Fetching from FHRS Open Data…"
            for id in "${LA_IDS[@]}"; do
              url="https://ratings.food.gov.uk/OpenDataFiles/FHRS${id}_en-GB.xml"
              echo "Downloading $url"
              curl -fsSL "$url" -o "data/raw/FHRS_${id}.xml"
            done
          else
            echo "XMLs already present; skipping fallback."
          fi
          ls -lh data/raw

      # 3) Guard: fail early if still no XMLs
      - name: Guard — ensure XMLs exist
        run: |
          test -n "$(ls -1 data/raw/*.xml 2>/dev/null)" || { echo "❌ No FHRS XML files present in data/raw"; exit 1; }

      - name: Snapshot previous LA totals (if any)
        run: |
          mkdir -p data
          if [ -f data/la_totals_last.json ]; then
            cp data/la_totals_last.json data/__prev_la_totals.json
          fi

      - name: Process data and identify new businesses
        run: python process_fhrs_data.py

      - name: Build LA deltas (Top-10 growth/reductions)
        run: |
          python - <<'PY'
          import json, os
          from datetime import date

          CUR_PATH  = "data/la_totals_last.json"
          PREV_PATH = "data/__prev_la_totals.json"
          OUT_PATH  = "data/la_deltas_latest.json"

          def load(p):
            if not os.path.exists(p): return None
            with open(p, "r", encoding="utf-8") as f: return json.load(f)

          cur  = load(CUR_PATH)
          prev = load(PREV_PATH) or {}

          if not cur:
            os.makedirs("data", exist_ok=True)
            with open(OUT_PATH, "w", encoding="utf-8") as f:
              json.dump({"date": str(date.today()),
                         "by_sector": {k: {"growth": [], "reductions": []}
                           for k in ["MOBILE","RESTAURANT_CAFE","PUB_BAR","TAKEAWAY","HOTEL","OTHER"]}}, f)
            raise SystemExit(0)

          SECTORS = ["MOBILE","RESTAURANT_CAFE","PUB_BAR","TAKEAWAY","HOTEL","OTHER"]

          def normalize(d):
            if not d: return {}
            if set(d.keys()) & set(SECTORS):
              out = {}
              for s in SECTORS:
                for la, v in (d.get(s) or {}).items():
                  out.setdefault(la, {})[s] = int(v or 0)
              return out
            return d

          curN, prevN = normalize(cur), normalize(prev)

          by_sector = {}
          for s in SECTORS:
            growth, reductions = [], []
            for la in set(curN) | set(prevN):
              cv = int((curN.get(la) or {}).get(s, 0))
              pv = int((prevN.get(la) or {}).get(s, 0))
              d = cv - pv
              if d > 0:  growth.append({"la": la, "delta": d, "current": cv})
              if d < 0:  reductions.append({"la": la, "delta": d, "current": cv})
            growth.sort(key=lambda x: x["delta"], reverse=True)
            reductions.sort(key=lambda x: x["delta"])
            by_sector[s] = {"growth": growth[:10], "reductions": reductions[:10]}

          os.makedirs("data", exist_ok=True)
          with open(OUT_PATH, "w", encoding="utf-8") as f:
            json.dump({"date": str(date.today()), "by_sector": by_sector}, f, ensure_ascii=False)
          PY

      - name: Commit processed data
        run: |
          set -euo pipefail
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config user.name  "github-actions[bot]"
          git config --global --add safe.directory "$GITHUB_WORKSPACE"

          git add -A data || true
          if git diff --staged --quiet; then
            echo "No changes to commit."
            exit 0
          fi

          git commit -m "Update dashboard + per-LA deltas"
          git fetch origin
          git pull --rebase --autostash origin main || true
          git push origin HEAD:main

      - name: Summary
        run: |
          echo "✅ Update finished."
          ls -lh data/* 2>/dev/null || true